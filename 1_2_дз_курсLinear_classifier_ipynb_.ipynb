{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "1.2 дз курсLinear classifier.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdhkS_1FXcGD"
      },
      "source": [
        "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
        "\n",
        "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
        "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
        "\n",
        "В этом задании вы:\n",
        "- потренируетесь считать градиенты различных многомерных функций\n",
        "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
        "- реализуете процесс тренировки линейного классификатора\n",
        "- подберете параметры тренировки на практике\n",
        "\n",
        "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
        "http://cs231n.github.io/python-numpy-tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEoTA3VrXcGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c99f10-2b5a-4bb3-a8d7-2ba4c36817f8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X11xm-EJXcGI"
      },
      "source": [
        "from dataset import load_svhn, random_split_train_val\n",
        "from gradient_check import check_gradient\n",
        "from metrics import multiclass_accuracy \n",
        "import linear_classifer"
      ],
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcRFaTvkfPoM",
        "outputId": "3c3e4360-0daf-4bc8-8c3c-f5cb3824691c"
      },
      "source": [
        "!wget -c http://ufldl.stanford.edu/housenumbers/train_32x32.mat http://ufldl.stanford.edu/housenumbers/test_32x32.mat"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-25 10:02:15--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2021-08-25 10:02:16--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
            "Reusing existing connection to ufldl.stanford.edu:80.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDBlwR9SXcGJ"
      },
      "source": [
        "# Как всегда, первым делом загружаем данные\n",
        "\n",
        "Мы будем использовать все тот же SVHN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYNofVILXcGK"
      },
      "source": [
        "def prepare_for_linear_classifier(train_X, test_X):\n",
        "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    \n",
        "    # Subtract mean\n",
        "    mean_image = np.mean(train_flat, axis = 0)\n",
        "    train_flat -= mean_image\n",
        "    test_flat -= mean_image\n",
        "    \n",
        "    # Add another channel with ones as a bias term\n",
        "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
        "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
        "    return train_flat_with_ones, test_flat_with_ones\n",
        "    \n",
        "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
        "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
        "# Split train into train and val\n",
        "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhkrOwJ3XcGL"
      },
      "source": [
        "# Играемся с градиентами!\n",
        "\n",
        "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
        "\n",
        "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
        "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
        "```\n",
        "def f(x):\n",
        "    \"\"\"\n",
        "    Computes function and analytic gradient at x\n",
        "    \n",
        "    x: np array of float, input to the function\n",
        "    \n",
        "    Returns:\n",
        "    value: float, value of the function \n",
        "    grad: np array of float, same shape as x\n",
        "    \"\"\"\n",
        "    ...\n",
        "    \n",
        "    return value, grad\n",
        "```\n",
        "\n",
        "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
        "\n",
        "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
        "\n",
        "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
        "\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
        "\n",
        "Все функции приведенные в следующей клетке должны проходить gradient check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cr3uEzHf6b2"
      },
      "source": [
        "def check_gradient(f, x, delta=1e-5, tol = 1e-4):\n",
        "    '''\n",
        "    Checks the implementation of analytical gradient by comparing\n",
        "    it to numerical gradient using two-point formula\n",
        "    Arguments:\n",
        "      f: function that receives x and computes value and gradient\n",
        "      x: np array, initial point where gradient is checked\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Return:\n",
        "      bool indicating whether gradients match or not\n",
        "    '''\n",
        "    assert isinstance(x, np.ndarray)\n",
        "    assert x.dtype == np.float\n",
        "    \n",
        "    orig_x = x.copy()\n",
        "    fx, analytic_grad = f(x)\n",
        "    assert np.all(np.isclose(orig_x, x, tol)), \"Functions shouldn't modify input variables\"\n",
        "    assert analytic_grad.shape == x.shape\n",
        "    analytic_grad = analytic_grad.copy()\n",
        "\n",
        "    # We will go through every dimension of x and compute numeric\n",
        "    # derivative for it\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "        analytic_grad_at_ix = analytic_grad[ix]\n",
        "        l = x.copy()\n",
        "        l[ix] = l[ix]+tol\n",
        "        numeric_grad_at_ix = (f(l)[0]-f(x)[0])/tol\n",
        "        # TODO compute value of numeric gradient of f to idx\n",
        "        if not np.isclose(numeric_grad_at_ix, analytic_grad_at_ix, tol):\n",
        "            print(\"Gradients are different at %s. Analytic: %2.4f, Numeric: %2.4f\" % (ix, analytic_grad_at_ix, numeric_grad_at_ix))\n",
        "            return False\n",
        "\n",
        "        it.iternext()\n",
        "\n",
        "    print(\"Gradient check passed!\")\n",
        "    return True\n"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7gJqgRqP_yJ",
        "outputId": "e3e11b4c-9c64-43a9-b32f-4eab6eae3ba8"
      },
      "source": [
        "it = np.nditer(np.array([[3.0, 2.0], [1.0, 0.0]]), flags=['multi_index'], op_flags=['readwrite'])\n",
        "it.iternext()\n",
        "it.iternext()\n",
        "ix = it.multi_index\n",
        "ix"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VQdTpAD-XcGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f3f4c5-ba39-4515-8d77-35e8709745e7"
      },
      "source": [
        "# TODO: Implement check_gradient function in gradient_check.py\n",
        "# All the functions below should pass the gradient check\n",
        "\n",
        "def square(x):\n",
        "    return float(x*x), 2*x\n",
        "\n",
        "check_gradient(square, np.array([3.0]))\n",
        "\n",
        "def array_sum(x):\n",
        "    assert x.shape == (2,), x.shape\n",
        "    return np.sum(x), np.ones_like(x)\n",
        "\n",
        "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
        "\n",
        "def array_2d_sum(x):\n",
        "    assert x.shape == (2,2)\n",
        "    return np.sum(x), np.ones_like(x)\n",
        "\n",
        "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n",
            "Gradient check passed!\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5VVrJLcEeB6",
        "outputId": "7c86a0dd-b5e7-4290-ab04-ec87e5f6887d"
      },
      "source": [
        "np.ones_like( np.array([[3.0, 2.0], [1.0, 0.0]]))"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1.],\n",
              "       [1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8llsGpNXcGO"
      },
      "source": [
        "## Начинаем писать свои функции, считающие аналитический градиент\n",
        "\n",
        "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
        "\n",
        "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
        "\n",
        "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
        "```\n",
        "predictions -= np.max(predictions)\n",
        "```\n",
        "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Kd4iNYu8Wu"
      },
      "source": [
        "def softmax(predictions):\n",
        "    '''\n",
        "    Computes probabilities from scores\n",
        "    Arguments:\n",
        "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
        "        classifier output\n",
        "    Returns:\n",
        "      probs, np array of the same shape as predictions - \n",
        "        probability for every class, 0..1\n",
        "    '''\n",
        "    predictions = predictions.copy()\n",
        "    predictions -= np.max(predictions)\n",
        "    return np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "    # TODO implement softmax\n",
        "    # Your final implementation shouldn't have any loops"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is9WBUe5XcGS"
      },
      "source": [
        "# TODO Implement softmax and cross-entropy for single sample\n",
        "linear_classifer.softmax = softmax\n",
        "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
        "\n",
        "# Make sure it works for big numbers too!\n",
        "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
        "assert np.isclose(probs[0], 1.0)"
      ],
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXFIUbJ6XcGW"
      },
      "source": [
        "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
        "В общем виде cross-entropy определена следующим образом:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
        "\n",
        "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
        "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
        "\n",
        "Это позволяет реализовать функцию проще!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qoX4TZ6wzL-"
      },
      "source": [
        "def cross_entropy_loss(probs, target_index):\n",
        "    '''\n",
        "    Computes cross-entropy loss\n",
        "    Arguments:\n",
        "      probs, np array, shape is either (N) or (batch_size, N) -\n",
        "        probabilities for every class\n",
        "      target_index: np array of int, shape is (1) or (batch_size) -\n",
        "        index of the true class for given sample(s)\n",
        "    Returns:\n",
        "      loss: single value\n",
        "    '''\n",
        "    # TODO implement cross-entropy\n",
        "    # Your final implementation shouldn't have any loops\n",
        "    return np.mean(-np.log(probs).T[target_index])"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNkAPtg3XcGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb7d8d7-7291-48a5-c655-5af97a038371"
      },
      "source": [
        "linear_classifer.cross_entropy_loss = cross_entropy_loss\n",
        "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
        "linear_classifer.cross_entropy_loss(probs, 1)"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.006760443547122"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lZpNa6oXcGY"
      },
      "source": [
        "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
        "\n",
        "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
        "\n",
        "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS2w4HZ0xuWB"
      },
      "source": [
        "def softmax_with_cross_entropy(predictions, target_index):\n",
        "    '''\n",
        "    Computes softmax and cross-entropy loss for model predictions,\n",
        "    including the gradient\n",
        "    Arguments:\n",
        "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
        "        classifier output\n",
        "      target_index: np array of int, shape is (1) or (batch_size) -\n",
        "        index of the true class for given sample(s)\n",
        "    Returns:\n",
        "      loss, single value - cross-entropy loss\n",
        "      dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
        "    '''\n",
        "    # TODO implement softmax with cross-entropy\n",
        "    # Your final implementation shouldn't have any loops\n",
        "    zeros = np.zeros_like(predictions)\n",
        "    if (type(target_index) != int):\n",
        "      grad=linear_classifer.softmax(predictions)-target_index\n",
        "    else:\n",
        "      zeros[target_index] = 1\n",
        "      grad=linear_classifer.softmax(predictions)-zeros\n",
        "    loss = cross_entropy_loss(linear_classifer.softmax(predictions),target_index)\n",
        "    return loss, grad"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62DvS4JxXcGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19ab069-ea69-4ed3-c76d-d8eb1e6f2a88"
      },
      "source": [
        "linear_classifer.softmax_with_cross_entropy = softmax_with_cross_entropy \n",
        "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
        "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXCAu_81XcGa"
      },
      "source": [
        "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
        "\n",
        "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
        "\n",
        "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
        "\n",
        "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I094bwYkgZC"
      },
      "source": [
        "def softmax(predictions):\n",
        "    '''\n",
        "    Computes probabilities from scores\n",
        "    Arguments:\n",
        "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
        "        classifier output\n",
        "    Returns:\n",
        "      probs, np array of the same shape as predictions - \n",
        "        probability for every class, 0..1\n",
        "    '''\n",
        "    predictions = predictions.copy()\n",
        "    \n",
        "    if len(predictions.shape) >1:\n",
        "      predictions-=np.max(predictions,axis = 1).reshape(predictions.shape[0],-1)\n",
        "      return (np.exp(predictions).T/(np.sum(np.exp(predictions),axis=1)+1e-5).T).T\n",
        "    else:\n",
        "      predictions -= np.max(predictions)\n",
        "      return np.exp(predictions) / (np.sum(np.exp(predictions)+1e-5))"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmOR4OXrljZ5"
      },
      "source": [
        "linear_classifer.softmax = softmax\n",
        "def cross_entropy_loss(probs, target_index):\n",
        "    '''\n",
        "    Computes cross-entropy loss\n",
        "    Arguments:\n",
        "      probs, np array, shape is either (N) or (batch_size, N) -\n",
        "        probabilities for every class\n",
        "      target_index: np array of int, shape is (1) or (batch_size) -\n",
        "        index of the true class for given sample(s)\n",
        "    Returns:\n",
        "      loss: single value\n",
        "    '''\n",
        "    # TODO implement cross-entropy\n",
        "    # Your final implementation shouldn't have any loops\n",
        "    if len(probs.shape) > 1:\n",
        "       return np.mean(-np.log(np.diag(probs.T[target_index.reshape(-1)])))\n",
        "    else:  return np.mean(-np.log(probs)[target_index])"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TGIXMktn7Xp"
      },
      "source": [
        "import pandas as pd\n",
        "linear_classifer.cross_entropy_loss = cross_entropy_loss\n",
        "def softmax_with_cross_entropy(predictions, target_index):\n",
        "    '''\n",
        "    Computes softmax and cross-entropy loss for model predictions,\n",
        "    including the gradient\n",
        "    Arguments:\n",
        "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
        "        classifier output\n",
        "      target_index: np array of int, shape is (1) or (batch_size) -\n",
        "        index of the true class for given sample(s)\n",
        "    Returns:\n",
        "      loss, single value - cross-entropy loss\n",
        "      dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
        "    '''\n",
        "    softmax_ = linear_classifer.softmax(predictions)\n",
        "    if len(target_index)!=1:\n",
        "      y = np.zeros_like(predictions)\n",
        "      y[np.arange(predictions.shape[0]),target_index.reshape(-1)]=1\n",
        "      grad = (softmax_ - y)/predictions.shape[0]\n",
        "    else: \n",
        "      y = np.zeros_like(predictions)\n",
        "      y[target_index] = 1\n",
        "      grad = (softmax_ - y)\n",
        "    loss = cross_entropy_loss(softmax_,target_index)\n",
        "    return loss, grad"
      ],
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3JeKhKUmXcGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb55c47-b520-448f-ca8c-e99186de9202"
      },
      "source": [
        "# TODO Extend combined function so it can receive a 2d array with batch of samples\\\n",
        "linear_classifer.softmax_with_cross_entropy = softmax_with_cross_entropy \n",
        "np.random.seed(42)\n",
        "# Test batch_size = 1\n",
        "num_classes = 4\n",
        "batch_size = 1\n",
        "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)[0]\n",
        "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)[0]\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
        "\n",
        "# Test batch_size = 3\n",
        "num_classes = 4\n",
        "batch_size = 3\n",
        "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
        "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
        "\n",
        "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
        "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
        "assert np.all(np.isclose(probs[:, 0], 1.0))"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpLFDssK5rK9"
      },
      "source": [
        "def check_gr(f, x, delta=1e-5, tol = 1e-4):\n",
        "    '''\n",
        "    Checks the implementation of analytical gradient by comparing\n",
        "    it to numerical gradient using two-point formula\n",
        "    Arguments:\n",
        "      f: function that receives x and computes value and gradient\n",
        "      x: np array, initial point where gradient is checked\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Return:\n",
        "      bool indicating whether gradients match or not\n",
        "    '''\n",
        "    assert isinstance(x, np.ndarray)\n",
        "    assert x.dtype == np.float\n",
        "    \n",
        "    orig_x = x.copy()\n",
        "    fx, analytic_grad = f(x)\n",
        "    assert np.all(np.isclose(orig_x, x, tol)), \"Functions shouldn't modify input variables\"\n",
        "    assert analytic_grad.shape == x.shape\n",
        "    analytic_grad = analytic_grad.copy()\n",
        "\n",
        "    # We will go through every dimension of x and compute numeric\n",
        "    # derivative for it\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    qq = []\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "        analytic_grad_at_ix = analytic_grad[ix]\n",
        "        l = x.copy()\n",
        "        l[ix] = l[ix]+tol\n",
        "        numeric_grad_at_ix = (f(l)[0]-f(x)[0])/tol\n",
        "        # TODO compute value of numeric gradient of f to idx\n",
        "        qq.append(numeric_grad_at_ix)\n",
        "        it.iternext()\n",
        "\n",
        "    print(np.array(qq).reshape(x.shape[0],-1))\n",
        "    return True"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS8GjAswXcGb"
      },
      "source": [
        "### Наконец, реализуем сам линейный классификатор!\n",
        "\n",
        "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
        "\n",
        "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
        "\n",
        "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
        "\n",
        "`predictions = X * W`, где `*` - матричное умножение.\n",
        "\n",
        "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5kdo_eWCgFy"
      },
      "source": [
        "def linear_softmax(X, W, target_index):\n",
        "    '''\n",
        "    Performs linear classification and returns loss and gradient over W\n",
        "    Arguments:\n",
        "      X, np array, shape (num_batch, num_features) - batch of images\n",
        "      W, np array, shape (num_features, classes) - weights\n",
        "      target_index, np array, shape (num_batch) - index of target classes\n",
        "    Returns:\n",
        "      loss, single value - cross-entropy loss\n",
        "      gradient, np.array same shape as W - gradient of weight by loss\n",
        "    '''\n",
        "    predictions = np.dot(X, W)\n",
        "\n",
        "    # TODO implement prediction and gradient over W\n",
        "    # Your final implementation shouldn't have any loops\n",
        "    loss,dW =  softmax_with_cross_entropy(predictions,target_index)\n",
        "    \n",
        "    return loss, X.T@dW"
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVu9lWPdXcGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca19663d-149a-4779-d194-08668ca0c22a"
      },
      "source": [
        "linear_classifer.linear_softmax = linear_softmax\n",
        "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
        "batch_size = 2\n",
        "num_classes = 2\n",
        "num_features = 3\n",
        "np.random.seed(42)\n",
        "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
        "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
        "target_index = np.ones(batch_size, dtype=np.int)\n",
        "\n",
        "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
        "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV4aNtkxXcGc"
      },
      "source": [
        "### И теперь регуляризация\n",
        "\n",
        "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
        "\n",
        "Напомним, L2 regularization определяется как\n",
        "\n",
        "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
        "\n",
        "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h7taI_t_q2j"
      },
      "source": [
        "def l2_regularization(W, reg_strength):\n",
        "    '''\n",
        "    Computes L2 regularization loss on weights and its gradient\n",
        "    Arguments:\n",
        "      W, np array - weights\n",
        "      reg_strength - float value\n",
        "    Returns:\n",
        "      loss, single value - l2 regularization loss\n",
        "      gradient, np.array same shape as W - gradient of weight by l2 loss\n",
        "    '''\n",
        "    loss = reg_strength * np.sum(W**2)\n",
        "    # TODO: implement l2 regularization and gradient\n",
        "    # Your final implementation shouldn't have any loops\n",
        "    \n",
        "    grad = 2*W*reg_strength\n",
        "    return loss, grad"
      ],
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J20LMSVOXcGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77e084f-f8b9-4e42-86b1-b241d4e35879"
      },
      "source": [
        "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
        "linear_classifer.l2_regularization = l2_regularization\n",
        "linear_classifer.l2_regularization(W, 0.01)\n",
        "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmvGV6djXcGd"
      },
      "source": [
        "# Тренировка!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYqP87ZXXcGe"
      },
      "source": [
        "Градиенты в порядке, реализуем процесс тренировки!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctOWdq9JSOV1"
      },
      "source": [
        "def init(self, lear = 1e-7,reg = 1e-5):\n",
        "        self.W = None\n",
        "        self.learning_rate = lear\n",
        "        self.reg = reg"
      ],
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i6_SCUSSI5N"
      },
      "source": [
        "linear_classifer.LinearSoftmaxClassifier.__init__ = init"
      ],
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrKyL05-F130"
      },
      "source": [
        "def fit(self, X, y, batch_size=100, epochs=1):\n",
        "        '''\n",
        "        Trains linear classifier\n",
        "        \n",
        "        Arguments:\n",
        "          X, np array (num_samples, num_features) - training data\n",
        "          y, np array of int (num_samples) - labels\n",
        "          batch_size, int - batch size to use\n",
        "          learning_rate, float - learning rate for gradient descent\n",
        "          reg, float - L2 regularization strength\n",
        "          epochs, int - number of epochs\n",
        "        '''\n",
        "\n",
        "        learning_rate = self.learning_rate\n",
        "        reg = self.reg\n",
        "\n",
        "        num_train = X.shape[0]\n",
        "        num_features = X.shape[1]\n",
        "        num_classes = np.max(y)+1\n",
        "        if self.W is None:\n",
        "            self.W = 0.001 * np.random.randn(num_features, num_classes)\n",
        "\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            loss = 0\n",
        "            shuffled_indices = np.arange(num_train)\n",
        "            np.random.shuffle(shuffled_indices)\n",
        "            sections = np.arange(batch_size, num_train, batch_size)\n",
        "            batches_indices = np.array_split(shuffled_indices, sections)\n",
        "            # TODO implement generating batches from indices\n",
        "            # Compute loss and gradients\n",
        "            # Apply gradient to weights using learning rate\n",
        "            # Don't forget to add both cross-entropy loss\n",
        "            # and regularization!\n",
        "            \n",
        "            for i,indicies in enumerate(batches_indices):\n",
        "              loss_nonreg, grad_w_nonreg = linear_classifer.linear_softmax(X[indicies,:], self.W, y[indicies])\n",
        "              loss_reg, grad_w_reg = linear_classifer.l2_regularization(self.W, reg)\n",
        "              self.W-=learning_rate*(grad_w_nonreg+grad_w_reg)\n",
        "              loss += loss_nonreg+loss_reg\n",
        "\n",
        "            # end\n",
        "            print(\"Epoch %i, loss: %f\" % (epoch, loss))\n",
        "            loss_history.append(loss)\n",
        "        return loss_history"
      ],
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ji6C0RMBXcGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f65c977-3acd-4708-e987-63cbd386c6ab"
      },
      "source": [
        "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
        "linear_classifer.LinearSoftmaxClassifier.fit = fit\n",
        "classifier = linear_classifer.LinearSoftmaxClassifier(1e-3,reg=1e1)\n",
        "loss_history = classifier.fit(train_X, train_y, epochs=10,  batch_size=300)"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 74.520486\n",
            "Epoch 1, loss: 70.681537\n",
            "Epoch 2, loss: 69.539398\n",
            "Epoch 3, loss: 69.198731\n",
            "Epoch 4, loss: 69.098940\n",
            "Epoch 5, loss: 69.069143\n",
            "Epoch 6, loss: 69.059761\n",
            "Epoch 7, loss: 69.056987\n",
            "Epoch 8, loss: 69.056466\n",
            "Epoch 9, loss: 69.055732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJYeolOlXcGf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "92c1aaaf-85a2-49ed-de8b-382fa0e26dbc"
      },
      "source": [
        "# let's look at the loss history!\n",
        "plt.plot(loss_history)"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2b7bea0790>]"
            ]
          },
          "metadata": {},
          "execution_count": 314
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "091wbE7yMi-A"
      },
      "source": [
        "def predict(self, X):\n",
        "        '''\n",
        "        Produces classifier predictions on the set\n",
        "       \n",
        "        Arguments:\n",
        "          X, np array (test_samples, num_features)\n",
        "        Returns:\n",
        "          y_pred, np.array of int (test_samples)\n",
        "        '''\n",
        "        y_pred = np.zeros(X.shape[0], dtype=np.int)\n",
        "        y_pred = np.argmax(linear_classifer.softmax(X@self.W),axis = 1)\n",
        "        # TODO Implement class prediction\n",
        "        # Your final implementation shouldn't have any loops\n",
        "\n",
        "        return y_pred"
      ],
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKywVoXNjP_2"
      },
      "source": [
        "def multiclass_accuracy(prediction, ground_truth):\n",
        "  return sum([a == b for a, b in zip(prediction, ground_truth)])/len(ground_truth)"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY4idm_eXcGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856a2f2a-8169-45d4-ea55-ca241d9f4d57"
      },
      "source": [
        "# Let's check how it performs on validation set\n",
        "linear_classifer.LinearSoftmaxClassifier.predict = predict\n",
        "pred = classifier.predict(val_X)\n",
        "accuracy = multiclass_accuracy(pred, val_y)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# Now, let's train more and see if it performs better\n",
        "classifier.fit(train_X, train_y, epochs=100, batch_size=300)\n",
        "pred = classifier.predict(val_X)\n",
        "accuracy = multiclass_accuracy(pred, val_y)\n",
        "print(\"Accuracy after training for 100 epochs: \", accuracy)"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.127\n",
            "Epoch 0, loss: 69.056728\n",
            "Epoch 1, loss: 69.056008\n",
            "Epoch 2, loss: 69.055063\n",
            "Epoch 3, loss: 69.056238\n",
            "Epoch 4, loss: 69.056781\n",
            "Epoch 5, loss: 69.055940\n",
            "Epoch 6, loss: 69.056435\n",
            "Epoch 7, loss: 69.055352\n",
            "Epoch 8, loss: 69.056378\n",
            "Epoch 9, loss: 69.056105\n",
            "Epoch 10, loss: 69.055774\n",
            "Epoch 11, loss: 69.055604\n",
            "Epoch 12, loss: 69.056626\n",
            "Epoch 13, loss: 69.056039\n",
            "Epoch 14, loss: 69.056074\n",
            "Epoch 15, loss: 69.056006\n",
            "Epoch 16, loss: 69.056209\n",
            "Epoch 17, loss: 69.055931\n",
            "Epoch 18, loss: 69.056047\n",
            "Epoch 19, loss: 69.056451\n",
            "Epoch 20, loss: 69.055503\n",
            "Epoch 21, loss: 69.055945\n",
            "Epoch 22, loss: 69.055487\n",
            "Epoch 23, loss: 69.056122\n",
            "Epoch 24, loss: 69.056102\n",
            "Epoch 25, loss: 69.055741\n",
            "Epoch 26, loss: 69.056354\n",
            "Epoch 27, loss: 69.056050\n",
            "Epoch 28, loss: 69.055511\n",
            "Epoch 29, loss: 69.055645\n",
            "Epoch 30, loss: 69.055892\n",
            "Epoch 31, loss: 69.055887\n",
            "Epoch 32, loss: 69.055612\n",
            "Epoch 33, loss: 69.056328\n",
            "Epoch 34, loss: 69.056007\n",
            "Epoch 35, loss: 69.055988\n",
            "Epoch 36, loss: 69.055770\n",
            "Epoch 37, loss: 69.055554\n",
            "Epoch 38, loss: 69.056179\n",
            "Epoch 39, loss: 69.055806\n",
            "Epoch 40, loss: 69.056002\n",
            "Epoch 41, loss: 69.055763\n",
            "Epoch 42, loss: 69.055911\n",
            "Epoch 43, loss: 69.056050\n",
            "Epoch 44, loss: 69.055950\n",
            "Epoch 45, loss: 69.056183\n",
            "Epoch 46, loss: 69.056068\n",
            "Epoch 47, loss: 69.056522\n",
            "Epoch 48, loss: 69.056141\n",
            "Epoch 49, loss: 69.055781\n",
            "Epoch 50, loss: 69.055724\n",
            "Epoch 51, loss: 69.055967\n",
            "Epoch 52, loss: 69.056123\n",
            "Epoch 53, loss: 69.056128\n",
            "Epoch 54, loss: 69.055919\n",
            "Epoch 55, loss: 69.055451\n",
            "Epoch 56, loss: 69.056214\n",
            "Epoch 57, loss: 69.056184\n",
            "Epoch 58, loss: 69.056069\n",
            "Epoch 59, loss: 69.055551\n",
            "Epoch 60, loss: 69.055840\n",
            "Epoch 61, loss: 69.055797\n",
            "Epoch 62, loss: 69.056028\n",
            "Epoch 63, loss: 69.055966\n",
            "Epoch 64, loss: 69.056172\n",
            "Epoch 65, loss: 69.056092\n",
            "Epoch 66, loss: 69.055477\n",
            "Epoch 67, loss: 69.055826\n",
            "Epoch 68, loss: 69.055938\n",
            "Epoch 69, loss: 69.055837\n",
            "Epoch 70, loss: 69.055946\n",
            "Epoch 71, loss: 69.056228\n",
            "Epoch 72, loss: 69.055569\n",
            "Epoch 73, loss: 69.056175\n",
            "Epoch 74, loss: 69.056688\n",
            "Epoch 75, loss: 69.056103\n",
            "Epoch 76, loss: 69.056335\n",
            "Epoch 77, loss: 69.056156\n",
            "Epoch 78, loss: 69.055960\n",
            "Epoch 79, loss: 69.056021\n",
            "Epoch 80, loss: 69.056159\n",
            "Epoch 81, loss: 69.056057\n",
            "Epoch 82, loss: 69.055902\n",
            "Epoch 83, loss: 69.055596\n",
            "Epoch 84, loss: 69.056323\n",
            "Epoch 85, loss: 69.055698\n",
            "Epoch 86, loss: 69.056069\n",
            "Epoch 87, loss: 69.055971\n",
            "Epoch 88, loss: 69.055924\n",
            "Epoch 89, loss: 69.056375\n",
            "Epoch 90, loss: 69.055957\n",
            "Epoch 91, loss: 69.056207\n",
            "Epoch 92, loss: 69.055875\n",
            "Epoch 93, loss: 69.056192\n",
            "Epoch 94, loss: 69.056539\n",
            "Epoch 95, loss: 69.056344\n",
            "Epoch 96, loss: 69.056128\n",
            "Epoch 97, loss: 69.056115\n",
            "Epoch 98, loss: 69.056978\n",
            "Epoch 99, loss: 69.055903\n",
            "Accuracy after training for 100 epochs:  0.121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yprCTrYLXcGg"
      },
      "source": [
        "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
        "\n",
        "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
        "\n",
        "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
        "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlVvu3lQk1O2",
        "outputId": "87be40a1-a958-4455-dd9f-03829dbe82e4"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "num_epochs = 200\n",
        "batch_size = 300\n",
        "\n",
        "learning_rates = [1e-2,1e-3, 1e-4]\n",
        "reg_strengths = [1e-2,1e-3,1e-4]\n",
        "\n",
        "# TODO use validation set to find the best hyperparameters\n",
        "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
        "# than provided initially\n",
        "accuracy = 0\n",
        "best_classifier = None\n",
        "best_val_accuracy = 0\n",
        "best_reg = 0\n",
        "best_lr = 0\n",
        "for lr in learning_rates:\n",
        "  for reg in reg_strengths:\n",
        "    clf = linear_classifer.LinearSoftmaxClassifier(lr,reg=reg)\n",
        "    clf.fit(train_X, train_y,epochs=15,batch_size=300)\n",
        "    accuracy = multiclass_accuracy(clf.predict(val_X), val_y)\n",
        "    if accuracy>best_val_accuracy:\n",
        "      best_lr = lr\n",
        "      best_reg = reg\n",
        "      best_val_accuracy = accuracy\n",
        "print('best validation accuracy achieved: %f' % best_val_accuracy , f'with best lr = {best_lr} and best reg = {reg}')"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 68.988784\n",
            "Epoch 1, loss: 68.735996\n",
            "Epoch 2, loss: 68.512316\n",
            "Epoch 3, loss: 68.301110\n",
            "Epoch 4, loss: 68.108784\n",
            "Epoch 5, loss: 67.938688\n",
            "Epoch 6, loss: 67.763803\n",
            "Epoch 7, loss: 67.617016\n",
            "Epoch 8, loss: 67.474918\n",
            "Epoch 9, loss: 67.332475\n",
            "Epoch 10, loss: 67.216729\n",
            "Epoch 11, loss: 67.097946\n",
            "Epoch 12, loss: 66.986568\n",
            "Epoch 13, loss: 66.895757\n",
            "Epoch 14, loss: 66.798842\n",
            "Epoch 0, loss: 68.985426\n",
            "Epoch 1, loss: 68.735395\n",
            "Epoch 2, loss: 68.500275\n",
            "Epoch 3, loss: 68.283524\n",
            "Epoch 4, loss: 68.080251\n",
            "Epoch 5, loss: 67.895419\n",
            "Epoch 6, loss: 67.721133\n",
            "Epoch 7, loss: 67.552364\n",
            "Epoch 8, loss: 67.395576\n",
            "Epoch 9, loss: 67.251536\n",
            "Epoch 10, loss: 67.113564\n",
            "Epoch 11, loss: 66.984199\n",
            "Epoch 12, loss: 66.864211\n",
            "Epoch 13, loss: 66.747352\n",
            "Epoch 14, loss: 66.628112\n",
            "Epoch 0, loss: 68.985369\n",
            "Epoch 1, loss: 68.730485\n",
            "Epoch 2, loss: 68.506237\n",
            "Epoch 3, loss: 68.278958\n",
            "Epoch 4, loss: 68.078535\n",
            "Epoch 5, loss: 67.890983\n",
            "Epoch 6, loss: 67.711309\n",
            "Epoch 7, loss: 67.545421\n",
            "Epoch 8, loss: 67.395109\n",
            "Epoch 9, loss: 67.249231\n",
            "Epoch 10, loss: 67.109056\n",
            "Epoch 11, loss: 66.974799\n",
            "Epoch 12, loss: 66.850490\n",
            "Epoch 13, loss: 66.736163\n",
            "Epoch 14, loss: 66.621273\n",
            "Epoch 0, loss: 69.072586\n",
            "Epoch 1, loss: 69.040453\n",
            "Epoch 2, loss: 69.010301\n",
            "Epoch 3, loss: 68.981123\n",
            "Epoch 4, loss: 68.953726\n",
            "Epoch 5, loss: 68.926478\n",
            "Epoch 6, loss: 68.899855\n",
            "Epoch 7, loss: 68.873276\n",
            "Epoch 8, loss: 68.848025\n",
            "Epoch 9, loss: 68.822774\n",
            "Epoch 10, loss: 68.798384\n",
            "Epoch 11, loss: 68.772901\n",
            "Epoch 12, loss: 68.748406\n",
            "Epoch 13, loss: 68.724676\n",
            "Epoch 14, loss: 68.700303\n",
            "Epoch 0, loss: 69.069260\n",
            "Epoch 1, loss: 69.037548\n",
            "Epoch 2, loss: 69.007623\n",
            "Epoch 3, loss: 68.980293\n",
            "Epoch 4, loss: 68.951674\n",
            "Epoch 5, loss: 68.924830\n",
            "Epoch 6, loss: 68.898410\n",
            "Epoch 7, loss: 68.872453\n",
            "Epoch 8, loss: 68.846308\n",
            "Epoch 9, loss: 68.821366\n",
            "Epoch 10, loss: 68.795362\n",
            "Epoch 11, loss: 68.771005\n",
            "Epoch 12, loss: 68.745842\n",
            "Epoch 13, loss: 68.721470\n",
            "Epoch 14, loss: 68.696886\n",
            "Epoch 0, loss: 69.066514\n",
            "Epoch 1, loss: 69.034493\n",
            "Epoch 2, loss: 69.005259\n",
            "Epoch 3, loss: 68.976300\n",
            "Epoch 4, loss: 68.948612\n",
            "Epoch 5, loss: 68.921144\n",
            "Epoch 6, loss: 68.894763\n",
            "Epoch 7, loss: 68.867753\n",
            "Epoch 8, loss: 68.842276\n",
            "Epoch 9, loss: 68.816937\n",
            "Epoch 10, loss: 68.791800\n",
            "Epoch 11, loss: 68.766572\n",
            "Epoch 12, loss: 68.741642\n",
            "Epoch 13, loss: 68.717891\n",
            "Epoch 14, loss: 68.692559\n",
            "Epoch 0, loss: 69.093031\n",
            "Epoch 1, loss: 69.089661\n",
            "Epoch 2, loss: 69.086354\n",
            "Epoch 3, loss: 69.083022\n",
            "Epoch 4, loss: 69.079751\n",
            "Epoch 5, loss: 69.076499\n",
            "Epoch 6, loss: 69.073208\n",
            "Epoch 7, loss: 69.070013\n",
            "Epoch 8, loss: 69.066800\n",
            "Epoch 9, loss: 69.063684\n",
            "Epoch 10, loss: 69.060449\n",
            "Epoch 11, loss: 69.057366\n",
            "Epoch 12, loss: 69.054226\n",
            "Epoch 13, loss: 69.051155\n",
            "Epoch 14, loss: 69.048066\n",
            "Epoch 0, loss: 69.080271\n",
            "Epoch 1, loss: 69.076924\n",
            "Epoch 2, loss: 69.073546\n",
            "Epoch 3, loss: 69.070246\n",
            "Epoch 4, loss: 69.067010\n",
            "Epoch 5, loss: 69.063658\n",
            "Epoch 6, loss: 69.060466\n",
            "Epoch 7, loss: 69.057297\n",
            "Epoch 8, loss: 69.054089\n",
            "Epoch 9, loss: 69.050899\n",
            "Epoch 10, loss: 69.047754\n",
            "Epoch 11, loss: 69.044632\n",
            "Epoch 12, loss: 69.041561\n",
            "Epoch 13, loss: 69.038444\n",
            "Epoch 14, loss: 69.035358\n",
            "Epoch 0, loss: 69.092796\n",
            "Epoch 1, loss: 69.089325\n",
            "Epoch 2, loss: 69.085892\n",
            "Epoch 3, loss: 69.082481\n",
            "Epoch 4, loss: 69.079067\n",
            "Epoch 5, loss: 69.075676\n",
            "Epoch 6, loss: 69.072316\n",
            "Epoch 7, loss: 69.069019\n",
            "Epoch 8, loss: 69.065811\n",
            "Epoch 9, loss: 69.062499\n",
            "Epoch 10, loss: 69.059260\n",
            "Epoch 11, loss: 69.056065\n",
            "Epoch 12, loss: 69.052853\n",
            "Epoch 13, loss: 69.049657\n",
            "Epoch 14, loss: 69.046586\n",
            "best validation accuracy achieved: 0.227000 with best lr = 0.01 and best reg = 0.0001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E9Nt3f0XcGg"
      },
      "source": [
        "# Какой же точности мы добились на тестовых данных?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9PlejAuXcGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0323b9c-8e4c-4f98-9208-16df5f7f26dc"
      },
      "source": [
        "best_classifier = linear_classifer.LinearSoftmaxClassifier(best_lr,best_reg)\n",
        "best_classifier.fit(train_X, train_y,epochs=150)\n",
        "test_pred = best_classifier.predict(test_X)\n",
        "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
        "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 206.475090\n",
            "Epoch 1, loss: 204.594985\n",
            "Epoch 2, loss: 203.132373\n",
            "Epoch 3, loss: 201.878113\n",
            "Epoch 4, loss: 200.915224\n",
            "Epoch 5, loss: 200.118965\n",
            "Epoch 6, loss: 199.419108\n",
            "Epoch 7, loss: 198.953081\n",
            "Epoch 8, loss: 198.439382\n",
            "Epoch 9, loss: 198.055997\n",
            "Epoch 10, loss: 197.729386\n",
            "Epoch 11, loss: 197.465632\n",
            "Epoch 12, loss: 197.202683\n",
            "Epoch 13, loss: 196.951567\n",
            "Epoch 14, loss: 196.830849\n",
            "Epoch 15, loss: 196.685086\n",
            "Epoch 16, loss: 196.516022\n",
            "Epoch 17, loss: 196.391301\n",
            "Epoch 18, loss: 196.288313\n",
            "Epoch 19, loss: 196.104944\n",
            "Epoch 20, loss: 196.050110\n",
            "Epoch 21, loss: 195.965453\n",
            "Epoch 22, loss: 195.896124\n",
            "Epoch 23, loss: 195.869619\n",
            "Epoch 24, loss: 195.769219\n",
            "Epoch 25, loss: 195.682437\n",
            "Epoch 26, loss: 195.632511\n",
            "Epoch 27, loss: 195.568217\n",
            "Epoch 28, loss: 195.543016\n",
            "Epoch 29, loss: 195.489228\n",
            "Epoch 30, loss: 195.479721\n",
            "Epoch 31, loss: 195.394469\n",
            "Epoch 32, loss: 195.407284\n",
            "Epoch 33, loss: 195.335396\n",
            "Epoch 34, loss: 195.320551\n",
            "Epoch 35, loss: 195.293985\n",
            "Epoch 36, loss: 195.241540\n",
            "Epoch 37, loss: 195.219939\n",
            "Epoch 38, loss: 195.222973\n",
            "Epoch 39, loss: 195.183110\n",
            "Epoch 40, loss: 195.183678\n",
            "Epoch 41, loss: 195.094778\n",
            "Epoch 42, loss: 195.159435\n",
            "Epoch 43, loss: 195.083227\n",
            "Epoch 44, loss: 195.083023\n",
            "Epoch 45, loss: 195.101844\n",
            "Epoch 46, loss: 195.063178\n",
            "Epoch 47, loss: 195.028938\n",
            "Epoch 48, loss: 195.041048\n",
            "Epoch 49, loss: 195.031616\n",
            "Epoch 50, loss: 195.026690\n",
            "Epoch 51, loss: 195.013252\n",
            "Epoch 52, loss: 195.019385\n",
            "Epoch 53, loss: 194.968766\n",
            "Epoch 54, loss: 194.979598\n",
            "Epoch 55, loss: 194.978308\n",
            "Epoch 56, loss: 194.948196\n",
            "Epoch 57, loss: 194.929041\n",
            "Epoch 58, loss: 194.938633\n",
            "Epoch 59, loss: 194.960960\n",
            "Epoch 60, loss: 194.902351\n",
            "Epoch 61, loss: 194.906108\n",
            "Epoch 62, loss: 194.860967\n",
            "Epoch 63, loss: 194.909473\n",
            "Epoch 64, loss: 194.899372\n",
            "Epoch 65, loss: 194.921497\n",
            "Epoch 66, loss: 194.920775\n",
            "Epoch 67, loss: 194.866988\n",
            "Epoch 68, loss: 194.897047\n",
            "Epoch 69, loss: 194.883491\n",
            "Epoch 70, loss: 194.856586\n",
            "Epoch 71, loss: 194.880504\n",
            "Epoch 72, loss: 194.846472\n",
            "Epoch 73, loss: 194.905225\n",
            "Epoch 74, loss: 194.822638\n",
            "Epoch 75, loss: 194.860021\n",
            "Epoch 76, loss: 194.855050\n",
            "Epoch 77, loss: 194.836421\n",
            "Epoch 78, loss: 194.860254\n",
            "Epoch 79, loss: 194.845507\n",
            "Epoch 80, loss: 194.849709\n",
            "Epoch 81, loss: 194.825834\n",
            "Epoch 82, loss: 194.830647\n",
            "Epoch 83, loss: 194.828266\n",
            "Epoch 84, loss: 194.833384\n",
            "Epoch 85, loss: 194.804058\n",
            "Epoch 86, loss: 194.837756\n",
            "Epoch 87, loss: 194.867608\n",
            "Epoch 88, loss: 194.859737\n",
            "Epoch 89, loss: 194.814546\n",
            "Epoch 90, loss: 194.813966\n",
            "Epoch 91, loss: 194.862209\n",
            "Epoch 92, loss: 194.776165\n",
            "Epoch 93, loss: 194.809048\n",
            "Epoch 94, loss: 194.837257\n",
            "Epoch 95, loss: 194.828106\n",
            "Epoch 96, loss: 194.816293\n",
            "Epoch 97, loss: 194.796164\n",
            "Epoch 98, loss: 194.824499\n",
            "Epoch 99, loss: 194.750101\n",
            "Epoch 100, loss: 194.842678\n",
            "Epoch 101, loss: 194.797663\n",
            "Epoch 102, loss: 194.804413\n",
            "Epoch 103, loss: 194.832780\n",
            "Epoch 104, loss: 194.817952\n",
            "Epoch 105, loss: 194.814497\n",
            "Epoch 106, loss: 194.777516\n",
            "Epoch 107, loss: 194.812916\n",
            "Epoch 108, loss: 194.862968\n",
            "Epoch 109, loss: 194.829443\n",
            "Epoch 110, loss: 194.802522\n",
            "Epoch 111, loss: 194.789221\n",
            "Epoch 112, loss: 194.760643\n",
            "Epoch 113, loss: 194.804437\n",
            "Epoch 114, loss: 194.812545\n",
            "Epoch 115, loss: 194.812935\n",
            "Epoch 116, loss: 194.765784\n",
            "Epoch 117, loss: 194.821267\n",
            "Epoch 118, loss: 194.808455\n",
            "Epoch 119, loss: 194.815925\n",
            "Epoch 120, loss: 194.762686\n",
            "Epoch 121, loss: 194.826427\n",
            "Epoch 122, loss: 194.813833\n",
            "Epoch 123, loss: 194.805290\n",
            "Epoch 124, loss: 194.769437\n",
            "Epoch 125, loss: 194.794462\n",
            "Epoch 126, loss: 194.824130\n",
            "Epoch 127, loss: 194.772909\n",
            "Epoch 128, loss: 194.750006\n",
            "Epoch 129, loss: 194.807309\n",
            "Epoch 130, loss: 194.787090\n",
            "Epoch 131, loss: 194.828281\n",
            "Epoch 132, loss: 194.774624\n",
            "Epoch 133, loss: 194.808826\n",
            "Epoch 134, loss: 194.786868\n",
            "Epoch 135, loss: 194.790082\n",
            "Epoch 136, loss: 194.813756\n",
            "Epoch 137, loss: 194.759140\n",
            "Epoch 138, loss: 194.803661\n",
            "Epoch 139, loss: 194.763659\n",
            "Epoch 140, loss: 194.820986\n",
            "Epoch 141, loss: 194.795116\n",
            "Epoch 142, loss: 194.780786\n",
            "Epoch 143, loss: 194.799947\n",
            "Epoch 144, loss: 194.797434\n",
            "Epoch 145, loss: 194.797088\n",
            "Epoch 146, loss: 194.781972\n",
            "Epoch 147, loss: 194.823579\n",
            "Epoch 148, loss: 194.788453\n",
            "Epoch 149, loss: 194.801877\n",
            "Linear softmax classifier test set accuracy: 0.204000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}